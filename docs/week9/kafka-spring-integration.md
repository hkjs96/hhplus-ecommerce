# Spring Bootì™€ Kafka ì—°ë™

> **ëª©í‘œ**: Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ Kafka Producer/Consumerë¥¼ êµ¬í˜„í•˜ê³ , íŠ¸ëœì­ì…˜ê³¼ ì—°ë™í•˜ëŠ” ë°©ë²•ì„ ìµíŒë‹¤.

---

## ğŸ“‹ ëª©ì°¨

1. [ì˜ì¡´ì„± ì„¤ì •](#ì˜ì¡´ì„±-ì„¤ì •)
2. [Kafka Configuration](#kafka-configuration)
3. [Producer êµ¬í˜„](#producer-êµ¬í˜„)
4. [Consumer êµ¬í˜„](#consumer-êµ¬í˜„)
5. [íŠ¸ëœì­ì…˜ ì—°ë™](#íŠ¸ëœì­ì…˜-ì—°ë™)
6. [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
7. [í…ŒìŠ¤íŠ¸](#í…ŒìŠ¤íŠ¸)

---

## ì˜ì¡´ì„± ì„¤ì •

### build.gradle

```gradle
dependencies {
    // Spring Kafka
    implementation 'org.springframework.kafka:spring-kafka'

    // JSON ì§ë ¬í™” (ì„ íƒ)
    implementation 'com.fasterxml.jackson.core:jackson-databind'

    // í…ŒìŠ¤íŠ¸
    testImplementation 'org.springframework.kafka:spring-kafka-test'
    testImplementation 'org.testcontainers:kafka'
}
```

### application.yml

```yaml
spring:
  kafka:
    # ê³µí†µ ì„¤ì •
    bootstrap-servers: localhost:9092

    # Producer ì„¤ì •
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all  # ëª¨ë“  Replica í™•ì¸
      retries: 3  # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
      properties:
        linger.ms: 10  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„
        batch.size: 16384  # ë°°ì¹˜ í¬ê¸°

    # Consumer ì„¤ì •
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      group-id: ecommerce-service
      auto-offset-reset: earliest  # Offset ì—†ì„ ë•Œ ì²˜ìŒë¶€í„°
      enable-auto-commit: false  # ìˆ˜ë™ Commit
      properties:
        spring.json.trusted.packages: "*"  # ì—­ì§ë ¬í™” í—ˆìš© íŒ¨í‚¤ì§€

    # Listener ì„¤ì •
    listener:
      ack-mode: manual  # ìˆ˜ë™ ACK
      concurrency: 3  # ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜

# ë¡œê¹…
logging:
  level:
    org.apache.kafka: INFO
    org.springframework.kafka: DEBUG
```

---

## Kafka Configuration

### KafkaProducerConfig.java

```java
package io.hhplus.ecommerce.config;

import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaProducerConfig {

    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;

    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> config = new HashMap<>();

        // Kafka ì„œë²„ ì£¼ì†Œ
        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);

        // Serializer ì„¤ì •
        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);

        // ACK ì„¤ì • (all = ëª¨ë“  Replica í™•ì¸)
        config.put(ProducerConfig.ACKS_CONFIG, "all");

        // ì¬ì‹œë„ ì„¤ì •
        config.put(ProducerConfig.RETRIES_CONFIG, 3);

        // Idempotence (ë©±ë“±ì„± ë³´ì¥)
        config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);

        // ë°°ì¹˜ ì„¤ì •
        config.put(ProducerConfig.LINGER_MS_CONFIG, 10);
        config.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);

        return new DefaultKafkaProducerFactory<>(config);
    }

    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```

### KafkaConsumerConfig.java

```java
package io.hhplus.ecommerce.config;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.support.serializer.JsonDeserializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConsumerConfig {

    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;

    @Value("${spring.kafka.consumer.group-id}")
    private String groupId;

    @Bean
    public ConsumerFactory<String, Object> consumerFactory() {
        Map<String, Object> config = new HashMap<>();

        // Kafka ì„œë²„ ì£¼ì†Œ
        config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);

        // Consumer Group ID
        config.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);

        // Deserializer ì„¤ì •
        config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);

        // Offset Reset ì •ì±…
        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        // Auto Commit ë¹„í™œì„±í™” (ìˆ˜ë™ Commit)
        config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);

        // JSON Deserializer ì„¤ì •
        config.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        config.put(JsonDeserializer.USE_TYPE_INFO_HEADERS, false);
        config.put(JsonDeserializer.VALUE_DEFAULT_TYPE, Object.class);

        return new DefaultKafkaConsumerFactory<>(config);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Object> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory =
                new ConcurrentKafkaListenerContainerFactory<>();

        factory.setConsumerFactory(consumerFactory());

        // ìˆ˜ë™ ACK ëª¨ë“œ
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);

        // ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
        factory.setConcurrency(3);

        // ë°°ì¹˜ ì²˜ë¦¬ (ì„ íƒ)
        // factory.setBatchListener(true);

        return factory;
    }
}
```

### KafkaTopics.java (Topic ìƒìˆ˜ ê´€ë¦¬)

```java
package io.hhplus.ecommerce.config;

public final class KafkaTopics {

    private KafkaTopics() {}

    // ì£¼ë¬¸ ê´€ë ¨
    public static final String ORDER_COMPLETED = "order-completed";
    public static final String ORDER_CANCELLED = "order-cancelled";

    // ê²°ì œ ê´€ë ¨
    public static final String PAYMENT_COMPLETED = "payment-completed";

    // ì¿ í° ê´€ë ¨
    public static final String COUPON_ISSUED = "coupon-issued";

    // ìƒí’ˆ ê´€ë ¨
    public static final String PRODUCT_STOCK_CHANGED = "product-stock-changed";

    // ì•Œë¦¼ ê´€ë ¨
    public static final String NOTIFICATION_REQUEST = "notification-request";
}
```

---

## Producer êµ¬í˜„

### 1. ë©”ì‹œì§€ DTO

```java
package io.hhplus.ecommerce.infrastructure.kafka.message;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Getter;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;

@Getter
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class OrderCompletedMessage {

    private String orderId;
    private String userId;
    private Long totalAmount;
    private LocalDateTime completedAt;

    public static OrderCompletedMessage from(Order order) {
        return OrderCompletedMessage.builder()
                .orderId(order.getId())
                .userId(order.getUserId())
                .totalAmount(order.getTotalAmount())
                .completedAt(LocalDateTime.now())
                .build();
    }
}
```

### 2. Producer êµ¬í˜„

```java
package io.hhplus.ecommerce.infrastructure.kafka.producer;

import io.hhplus.ecommerce.config.KafkaTopics;
import io.hhplus.ecommerce.infrastructure.kafka.message.OrderCompletedMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.stereotype.Component;

import java.util.concurrent.CompletableFuture;

@Slf4j
@Component
@RequiredArgsConstructor
public class OrderEventProducer {

    private final KafkaTemplate<String, Object> kafkaTemplate;

    public void publishOrderCompleted(OrderCompletedMessage message) {
        String topic = KafkaTopics.ORDER_COMPLETED;
        String key = message.getOrderId();  // ì£¼ë¬¸ IDë¥¼ í‚¤ë¡œ ì‚¬ìš©

        log.info("Publishing order completed event: orderId={}", key);

        CompletableFuture<SendResult<String, Object>> future =
                kafkaTemplate.send(topic, key, message);

        future.whenComplete((result, ex) -> {
            if (ex == null) {
                log.info("Message sent successfully: topic={}, partition={}, offset={}",
                        result.getRecordMetadata().topic(),
                        result.getRecordMetadata().partition(),
                        result.getRecordMetadata().offset());
            } else {
                log.error("Failed to send message: topic={}, key={}", topic, key, ex);
            }
        });
    }

    // ë™ê¸° ë°©ì‹ (í…ŒìŠ¤íŠ¸ ë˜ëŠ” ì¤‘ìš”í•œ ë©”ì‹œì§€)
    public void publishOrderCompletedSync(OrderCompletedMessage message) {
        String topic = KafkaTopics.ORDER_COMPLETED;
        String key = message.getOrderId();

        try {
            SendResult<String, Object> result = kafkaTemplate.send(topic, key, message).get();
            log.info("Message sent successfully: topic={}, offset={}",
                    result.getRecordMetadata().topic(),
                    result.getRecordMetadata().offset());
        } catch (Exception e) {
            log.error("Failed to send message synchronously", e);
            throw new RuntimeException("Kafka message send failed", e);
        }
    }
}
```

---

## Consumer êµ¬í˜„

### 1. Consumer êµ¬í˜„

```java
package io.hhplus.ecommerce.infrastructure.kafka.consumer;

import io.hhplus.ecommerce.config.KafkaTopics;
import io.hhplus.ecommerce.infrastructure.kafka.message.OrderCompletedMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.kafka.support.KafkaHeaders;
import org.springframework.messaging.handler.annotation.Header;
import org.springframework.messaging.handler.annotation.Payload;
import org.springframework.stereotype.Component;

@Slf4j
@Component
@RequiredArgsConstructor
public class OrderEventConsumer {

    private final DataPlatformClient dataPlatformClient;

    @KafkaListener(
            topics = KafkaTopics.ORDER_COMPLETED,
            groupId = "data-platform",
            containerFactory = "kafkaListenerContainerFactory"
    )
    public void handleOrderCompleted(
            @Payload OrderCompletedMessage message,
            @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
            @Header(KafkaHeaders.OFFSET) long offset,
            Acknowledgment ack
    ) {
        log.info("Received order completed event: orderId={}, partition={}, offset={}",
                message.getOrderId(), partition, offset);

        try {
            // ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
            dataPlatformClient.sendOrderData(message);

            // ì²˜ë¦¬ ì„±ê³µ ì‹œ ACK
            ack.acknowledge();

            log.info("Order data sent to data platform: orderId={}", message.getOrderId());
        } catch (Exception e) {
            log.error("Failed to process order completed event: orderId={}",
                    message.getOrderId(), e);
            // ACK í•˜ì§€ ì•ŠìŒ â†’ ì¬ì²˜ë¦¬ë¨
        }
    }
}
```

### 2. ë°°ì¹˜ Consumer (ëŒ€ëŸ‰ ì²˜ë¦¬)

```java
package io.hhplus.ecommerce.infrastructure.kafka.consumer;

import io.hhplus.ecommerce.config.KafkaTopics;
import io.hhplus.ecommerce.infrastructure.kafka.message.OrderCompletedMessage;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Component;

import java.util.List;

@Slf4j
@Component
@RequiredArgsConstructor
public class OrderBatchConsumer {

    private final OrderService orderService;

    @KafkaListener(
            topics = KafkaTopics.ORDER_COMPLETED,
            groupId = "batch-processor",
            containerFactory = "batchKafkaListenerContainerFactory"
    )
    public void handleBatch(
            List<OrderCompletedMessage> messages,
            Acknowledgment ack
    ) {
        log.info("Received batch of {} orders", messages.size());

        try {
            // ë°°ì¹˜ ì²˜ë¦¬
            orderService.processBatch(messages);

            // ì „ì²´ ë°°ì¹˜ ì„±ê³µ ì‹œ ACK
            ack.acknowledge();

            log.info("Batch processed successfully: count={}", messages.size());
        } catch (Exception e) {
            log.error("Failed to process batch", e);
            // ACK í•˜ì§€ ì•ŠìŒ â†’ ì „ì²´ ì¬ì²˜ë¦¬
        }
    }
}
```

**ë°°ì¹˜ Consumer Factory ì„¤ì •**
```java
@Bean
public ConcurrentKafkaListenerContainerFactory<String, Object> batchKafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Object> factory =
            new ConcurrentKafkaListenerContainerFactory<>();

    factory.setConsumerFactory(consumerFactory());
    factory.setBatchListener(true);  // ë°°ì¹˜ ëª¨ë“œ
    factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);

    return factory;
}
```

---

## íŠ¸ëœì­ì…˜ ì—°ë™

### 1. Application Eventì™€ í†µí•©

**íŠ¸ëœì­ì…˜ ì»¤ë°‹ í›„ Kafka ë°œí–‰**

```java
package io.hhplus.ecommerce.application.order;

import io.hhplus.ecommerce.domain.order.Order;
import io.hhplus.ecommerce.domain.order.event.OrderCompletedEvent;
import lombok.RequiredArgsConstructor;
import org.springframework.context.ApplicationEventPublisher;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

@Service
@RequiredArgsConstructor
public class OrderUseCase {

    private final OrderRepository orderRepository;
    private final ApplicationEventPublisher eventPublisher;

    @Transactional
    public Order createOrder(CreateOrderCommand command) {
        // 1. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìˆ˜í–‰
        Order order = Order.create(command);
        orderRepository.save(order);

        // 2. ì´ë²¤íŠ¸ ë°œí–‰ (íŠ¸ëœì­ì…˜ ì»¤ë°‹ í›„ ì²˜ë¦¬ë¨)
        eventPublisher.publishEvent(new OrderCompletedEvent(order));

        return order;
    }
}
```

**Event Listener (AFTER_COMMIT)**

```java
package io.hhplus.ecommerce.infrastructure.kafka.listener;

import io.hhplus.ecommerce.domain.order.event.OrderCompletedEvent;
import io.hhplus.ecommerce.infrastructure.kafka.message.OrderCompletedMessage;
import io.hhplus.ecommerce.infrastructure.kafka.producer.OrderEventProducer;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;
import org.springframework.transaction.event.TransactionPhase;
import org.springframework.transaction.event.TransactionalEventListener;

@Slf4j
@Component
@RequiredArgsConstructor
public class OrderEventListener {

    private final OrderEventProducer orderEventProducer;

    @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT)
    public void handleOrderCompleted(OrderCompletedEvent event) {
        log.info("Transaction committed, publishing to Kafka: orderId={}",
                event.getOrder().getId());

        OrderCompletedMessage message = OrderCompletedMessage.from(event.getOrder());
        orderEventProducer.publishOrderCompleted(message);
    }
}
```

### 2. Transactional Outbox íŒ¨í„´ (ê³ ê¸‰)

**Outbox Entity**

```java
package io.hhplus.ecommerce.infrastructure.event;

import jakarta.persistence.*;
import lombok.AccessLevel;
import lombok.Getter;
import lombok.NoArgsConstructor;

import java.time.LocalDateTime;

@Entity
@Table(name = "event_outbox")
@Getter
@NoArgsConstructor(access = AccessLevel.PROTECTED)
public class EventOutbox {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false)
    private String aggregateType;  // ORDER, PAYMENT, COUPON

    @Column(nullable = false)
    private String aggregateId;

    @Column(nullable = false)
    private String eventType;  // ORDER_COMPLETED, PAYMENT_COMPLETED

    @Column(columnDefinition = "TEXT", nullable = false)
    private String payload;  // JSON

    @Enumerated(EnumType.STRING)
    @Column(nullable = false)
    private OutboxStatus status;  // INIT, PUBLISHED

    @Column(nullable = false)
    private LocalDateTime createdAt;

    private LocalDateTime publishedAt;

    public EventOutbox(String aggregateType, String aggregateId, String eventType, String payload) {
        this.aggregateType = aggregateType;
        this.aggregateId = aggregateId;
        this.eventType = eventType;
        this.payload = payload;
        this.status = OutboxStatus.INIT;
        this.createdAt = LocalDateTime.now();
    }

    public void markAsPublished() {
        this.status = OutboxStatus.PUBLISHED;
        this.publishedAt = LocalDateTime.now();
    }

    public enum OutboxStatus {
        INIT, PUBLISHED
    }
}
```

**Outbox Publisher (Scheduler)**

```java
package io.hhplus.ecommerce.infrastructure.event;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

@Slf4j
@Component
@RequiredArgsConstructor
public class OutboxPublisher {

    private final EventOutboxRepository outboxRepository;
    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Scheduled(fixedDelay = 5000)  // 5ì´ˆë§ˆë‹¤ ì‹¤í–‰
    @Transactional
    public void publishEvents() {
        List<EventOutbox> events = outboxRepository.findByStatus(EventOutbox.OutboxStatus.INIT);

        for (EventOutbox event : events) {
            try {
                // Kafka ë°œí–‰
                kafkaTemplate.send(event.getEventType(), event.getAggregateId(), event.getPayload()).get();

                // ìƒíƒœ ì—…ë°ì´íŠ¸
                event.markAsPublished();
                outboxRepository.save(event);

                log.info("Outbox event published: id={}, eventType={}", event.getId(), event.getEventType());
            } catch (Exception e) {
                log.error("Failed to publish outbox event: id={}", event.getId(), e);
            }
        }
    }
}
```

---

## ì—ëŸ¬ ì²˜ë¦¬

### 1. DLQ (Dead Letter Queue)

```java
package io.hhplus.ecommerce.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.util.backoff.FixedBackOff;

@Configuration
public class KafkaErrorHandlingConfig {

    @Bean
    public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> kafkaTemplate) {
        // DLQë¡œ ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì „ì†¡
        DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
                kafkaTemplate,
                (record, ex) -> {
                    // DLQ Topic ì´ë¦„: {ì›ë³¸ í† í”½}.DLT
                    return new org.apache.kafka.common.TopicPartition(
                            record.topic() + ".DLT",
                            record.partition()
                    );
                }
        );

        // ì¬ì‹œë„ ì„¤ì •: 3ë²ˆ, 1ì´ˆ ê°„ê²©
        DefaultErrorHandler errorHandler = new DefaultErrorHandler(
                recoverer,
                new FixedBackOff(1000L, 3)
        );

        // ì¬ì‹œë„í•˜ì§€ ì•Šì„ ì˜ˆì™¸
        errorHandler.addNotRetryableExceptions(IllegalArgumentException.class);

        return errorHandler;
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Object> kafkaListenerContainerFactory(
            ConsumerFactory<String, Object> consumerFactory,
            DefaultErrorHandler errorHandler
    ) {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory =
                new ConcurrentKafkaListenerContainerFactory<>();

        factory.setConsumerFactory(consumerFactory);
        factory.setCommonErrorHandler(errorHandler);

        return factory;
    }
}
```

### 2. DLQ Consumer

```java
package io.hhplus.ecommerce.infrastructure.kafka.consumer;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

@Slf4j
@Component
@RequiredArgsConstructor
public class DLQConsumer {

    private final FailedMessageRepository failedMessageRepository;

    @KafkaListener(
            topics = "order-completed.DLT",
            groupId = "dlq-handler"
    )
    public void handleDLQ(String message) {
        log.error("Received message from DLQ: {}", message);

        // DBì— ì €ì¥í•˜ì—¬ ìˆ˜ë™ ì¬ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ
        FailedMessage failedMessage = new FailedMessage(
                "order-completed",
                message,
                LocalDateTime.now()
        );
        failedMessageRepository.save(failedMessage);

        log.info("Failed message saved to DB: id={}", failedMessage.getId());
    }
}
```

---

## í…ŒìŠ¤íŠ¸

### 1. í†µí•© í…ŒìŠ¤íŠ¸ (Testcontainers)

```java
package io.hhplus.ecommerce.infrastructure.kafka;

import io.hhplus.ecommerce.config.KafkaTopics;
import io.hhplus.ecommerce.infrastructure.kafka.message.OrderCompletedMessage;
import io.hhplus.ecommerce.infrastructure.kafka.producer.OrderEventProducer;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.test.context.EmbeddedKafka;
import org.springframework.test.context.DynamicPropertyRegistry;
import org.springframework.test.context.DynamicPropertySource;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import static org.assertj.core.api.Assertions.assertThat;

@SpringBootTest
@Testcontainers
class KafkaIntegrationTest {

    @Container
    static KafkaContainer kafka = new KafkaContainer(
            DockerImageName.parse("confluentinc/cp-kafka:7.5.3")
    );

    @DynamicPropertySource
    static void kafkaProperties(DynamicPropertyRegistry registry) {
        registry.add("spring.kafka.bootstrap-servers", kafka::getBootstrapServers);
    }

    @Autowired
    private OrderEventProducer orderEventProducer;

    private CountDownLatch latch = new CountDownLatch(1);
    private OrderCompletedMessage receivedMessage;

    @Test
    void shouldPublishAndConsumeMessage() throws InterruptedException {
        // Given
        OrderCompletedMessage message = OrderCompletedMessage.builder()
                .orderId("order-123")
                .userId("user-456")
                .totalAmount(50000L)
                .build();

        // When
        orderEventProducer.publishOrderCompleted(message);

        // Then
        boolean messageReceived = latch.await(10, TimeUnit.SECONDS);
        assertThat(messageReceived).isTrue();
        assertThat(receivedMessage.getOrderId()).isEqualTo("order-123");
    }

    @KafkaListener(topics = KafkaTopics.ORDER_COMPLETED, groupId = "test-group")
    public void receive(OrderCompletedMessage message) {
        receivedMessage = message;
        latch.countDown();
    }
}
```

### 2. Producer ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```java
package io.hhplus.ecommerce.infrastructure.kafka.producer;

import io.hhplus.ecommerce.config.KafkaTopics;
import io.hhplus.ecommerce.infrastructure.kafka.message.OrderCompletedMessage;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;

import java.util.concurrent.CompletableFuture;

import static org.mockito.ArgumentMatchers.any;
import static org.mockito.ArgumentMatchers.eq;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;

@ExtendWith(MockitoExtension.class)
class OrderEventProducerTest {

    @Mock
    private KafkaTemplate<String, Object> kafkaTemplate;

    @InjectMocks
    private OrderEventProducer orderEventProducer;

    @Test
    void shouldPublishOrderCompletedEvent() {
        // Given
        OrderCompletedMessage message = OrderCompletedMessage.builder()
                .orderId("order-123")
                .userId("user-456")
                .totalAmount(50000L)
                .build();

        CompletableFuture<SendResult<String, Object>> future = CompletableFuture.completedFuture(null);
        when(kafkaTemplate.send(any(), any(), any())).thenReturn(future);

        // When
        orderEventProducer.publishOrderCompleted(message);

        // Then
        verify(kafkaTemplate).send(
                eq(KafkaTopics.ORDER_COMPLETED),
                eq("order-123"),
                eq(message)
        );
    }
}
```

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ê¸°ë³¸ ì„¤ì •
- [ ] `spring-kafka` ì˜ì¡´ì„± ì¶”ê°€
- [ ] `application.yml` Kafka ì„¤ì •
- [ ] Producer/Consumer Configuration ì‘ì„±
- [ ] Topic ìƒìˆ˜ ê´€ë¦¬ í´ë˜ìŠ¤ ì‘ì„±

### âœ… Producer êµ¬í˜„
- [ ] ë©”ì‹œì§€ DTO ì‘ì„±
- [ ] Producer í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ë¹„ë™ê¸° ë°œí–‰ ë° ì½œë°± ì²˜ë¦¬
- [ ] ë¡œê¹… ì¶”ê°€

### âœ… Consumer êµ¬í˜„
- [ ] Consumer í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ìˆ˜ë™ ACK ì²˜ë¦¬
- [ ] ì˜ˆì™¸ ì²˜ë¦¬
- [ ] ë¡œê¹… ì¶”ê°€

### âœ… íŠ¸ëœì­ì…˜ ì—°ë™
- [ ] `@TransactionalEventListener(AFTER_COMMIT)` ì‚¬ìš©
- [ ] Application Event ë°œí–‰
- [ ] Kafka ë°œí–‰ì„ Event Listenerì—ì„œ ì²˜ë¦¬

### âœ… ì—ëŸ¬ ì²˜ë¦¬
- [ ] DLQ ì„¤ì •
- [ ] DLQ Consumer êµ¬í˜„
- [ ] ì¬ì‹œë„ ì •ì±… ì„¤ì •
- [ ] ì‹¤íŒ¨ ë©”ì‹œì§€ ì €ì¥

### âœ… í…ŒìŠ¤íŠ¸
- [ ] Testcontainers í†µí•© í…ŒìŠ¤íŠ¸
- [ ] Producer ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] Consumer ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] DLQ í…ŒìŠ¤íŠ¸

---

## ë‹¤ìŒ ë‹¨ê³„

- [ ] [ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ê°œì„ ](./kafka-use-cases.md)
- [ ] [ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](./kafka-best-practices.md)

---

**Last Updated**: 2024-12-18
